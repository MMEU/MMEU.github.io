<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Modal Embedding and Understanding</title>
    <link rel="stylesheet" href="css/style.css">
</head>


<body>
    
    <div class="container" id="id1">
        <table border="0" align="center">
            <tr>
                <td width="700" align="center" valign="middle"><h3>ACM MM Asia 2021 Workshop on</h3>
                    <span class="title">Multi-Modal Embedding and Understanding</span></td>
            </tr>
            <tr>
            <td colspan="3" align="center"><h3>Gold Coast, Australia<br>December1-3, 2021</h3>
            </tr>
        </table>
        <br><p><img src="resources/image/gold_coast.jpg" width="1000" align="middle"></p>
    </div>


</br>
</br>

    <div class="container">
        <h2 class="title3">Overview</h2>
        <div class="overview">
            <p>Although computer vision models have achieved advanced performance on various recognition tasks in recent
                years, they are known to be vulnerable against adversarial examples. The existence of adversarial examples
                reveals that current computer vision models perform differently with the human vision system, and on the other
                hand provides opportunities for understanding and improving these models. </p>

            <p>In this workshop, we will focus on recent research and future directions on adversarial machine learning in
                computer vision. We aim to bring experts from the computer vision, machine learning and security communities
                together to highlight the recent progress in this area, as well as discuss the benefits of integrating
                recent progress in adversarial machine learning into general computer vision tasks. Specifically, we seek to
                study adversarial machine learning not only for enhancing the model robustness against adversarial attacks,
                but also as a guide to diagnose/explain the limitation of current computer vision models as well as
                potential improving strategies. We hope this workshop can shed light on bridging the gap between the human
                vision system and computer vision systems, and chart out cross-community collaborations, including computer
                vision, machine learning and security communities.</p>
        </div>
    </div>

</br>
</br>

    <div class="container">
        <h2 class="title3">Call for Papers</h2>
        <div class="overview">
            <p>
                Multi-modal understanding are important and fundamental problems in the field of 
                multimodal analysis, which have been attracting much research attention in recent 
                years. Previous works have explored shallow embedding and understanding in many 
                downstream tasks, including cross-modal retrieval, visual navigation, VQA, visual 
                captioning, etc. To encourage researchers to explore new and advanced techniques in 
                this area, we are organizing a workshop on “multi-modal embedding and understanding” 
                with the conjunction of ACM MM Asia 2021, and calling for contributions. The included 
                (but not limited) topics are as follows:
            </p>
            <p>
                <ol>
                    <li>Large-scale pre-training for multi-modal embedding and understanding</li>
                    <li>Self-supervised learning in multi-modal embedding and understanding</li>
                    <li>Semi-supervised learning in multi-modal embedding and understanding</li>
                    <li>Contrastive learning in multi-modal embedding and understanding</li>
                    <li>Interpretability in multi-modal embedding and understanding</li>
                    <li>Interactive multi-modal understanding</li>
                    <li>Trust AI for multi-modal understanding</li>
                    <li>Cross-modal matching and retrieval</li>
                    <li>Cross-modal understanding</li>
                    <li>Multi-modal deep fake generation and detection</li>
                    <li>And other related…</li>
                </ol>
            </p>
        </div>
    </div>

</br>
</br>

    <div class="container">
        <h2 class="title3">Submission Guidelines</h2>
        <div class="overview">
            <p>
                <span class="bold">Format</span>: Submitted papers (.pdf format) must use the ACM Article Template 
                <a href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>
                . Please remember to add Concepts and Keywords.
            </p>
            <p>
                <span class="bold">Length</span>: Papers must be <span class="bold">no longer than 6 pages</span>, including all text and figures, and 
                up to two additional pages may be added for references. The reference pages must 
                only contain references. Over-length papers will be rejected without review.
            </p>
        </div>
    </div>

</br>
</br>

    <div class="container">
        <h2 class="title3">Important dates</h2>
        <div class="overview">
            <p>
                <ul>
                    <li>Paper submission deadline: Oct 10, 2021, 23:59 AoE</li>
                    <li>Notifications of acceptance: Nov 1, 2021</li>
                    <li>Camera-ready submission: Nov 7, 2021, 23:59 AoE</li>
                </ul>
            </p>
        </div>
    </div>

</br>
</br>
        
    <div class="container">
        <h2 class="title3">Organisers</h2>
        <div class="overview">
            <p>
                <ul>
                    <li><span class="bold">Wenguan Wang</span>, ETH Zurich, Switzerland</li>
                    <li><span class="bold">Xiaojun Chang, RMIT</span>, Australia</li>
                    <li><span class="bold">Yanli Ji</span>, University of Electronic Science and Technology of China, China</li>
                    <li><span class="bold">Yi Bin</span>, University of Electronic Science and Technology of China, China</li>
                </ul>
            </p>
        </div>
    </div>

</body>
</html>